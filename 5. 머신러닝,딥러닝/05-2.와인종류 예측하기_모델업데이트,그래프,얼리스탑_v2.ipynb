{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\Desktop\\\\Data-Analysis\\\\5. Deep Learning'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "목표 : 레드와인과 화이트와인 구분하기"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "데이터 : UCI Machine Learning Repository (https://archive.ics.uci.edu)\n",
    "- 레드와인 1599개, 화이트와인 4898개를 합친 데이터 = 6,497개 행, 13개 컬럼 데이터\n",
    "- 12가지 features : 주석산 농도, 아세트산 농도, 구연산 농도, 잔류 당분 농도, 염화나트륨 농도, 유리 아황산 농도, 총 아황산 농도, 밀도, PH, 황산칼륨 농도, 알코올 도수, 와인의 맛(0~10등급)\n",
    "- class : 레드와인(1), 화이트와인(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "                                                #오버 학습 방지\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 값 설정 (환경을 같게 만들어주어, 딥러닝 실행시 같은 값이 나올 수 있게 해줌)\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 입력\n",
    "df_pre = pd.read_csv('./data/wine.csv', header=None)\n",
    "df_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5773</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>11.90</td>\n",
       "      <td>0.050</td>\n",
       "      <td>65.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0.99659</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5043</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.44</td>\n",
       "      <td>18.75</td>\n",
       "      <td>0.057</td>\n",
       "      <td>65.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.99956</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.034</td>\n",
       "      <td>36.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.99026</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.55</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>8.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.40</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.049</td>\n",
       "      <td>12.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.99660</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.84</td>\n",
       "      <td>9.7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>5.2</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.36</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.031</td>\n",
       "      <td>46.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.98970</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.31</td>\n",
       "      <td>12.4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3492</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.31</td>\n",
       "      <td>18.30</td>\n",
       "      <td>0.046</td>\n",
       "      <td>40.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.41</td>\n",
       "      <td>8.7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5819</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.046</td>\n",
       "      <td>29.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99033</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.53</td>\n",
       "      <td>11.3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.42</td>\n",
       "      <td>7.10</td>\n",
       "      <td>0.045</td>\n",
       "      <td>41.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.99590</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.43</td>\n",
       "      <td>10.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.30</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.027</td>\n",
       "      <td>29.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99085</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>7.7</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.36</td>\n",
       "      <td>4.30</td>\n",
       "      <td>0.026</td>\n",
       "      <td>15.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.99152</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.48</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2      3      4     5      6        7     8     9     10  \\\n",
       "5773  6.3  0.24  0.22  11.90  0.050  65.0  179.0  0.99659  3.06  0.58   9.3   \n",
       "5043  6.7  0.30  0.44  18.75  0.057  65.0  224.0  0.99956  3.11  0.53   9.1   \n",
       "4555  8.1  0.12  0.38   0.90  0.034  36.0   86.0  0.99026  2.80  0.55  12.0   \n",
       "1753  8.2  0.23  0.40   7.50  0.049  12.0   76.0  0.99660  3.06  0.84   9.7   \n",
       "4799  5.2  0.31  0.36   5.10  0.031  46.0  145.0  0.98970  3.14  0.31  12.4   \n",
       "...   ...   ...   ...    ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "3492  6.8  0.24  0.31  18.30  0.046  40.0  142.0  1.00000  3.30  0.41   8.7   \n",
       "5819  6.0  0.19  0.29   1.20  0.046  29.0   92.0  0.99033  3.22  0.53  11.3   \n",
       "2814  8.0  0.28  0.42   7.10  0.045  41.0  169.0  0.99590  3.17  0.43  10.6   \n",
       "4319  7.5  0.26  0.30   4.60  0.027  29.0   92.0  0.99085  3.15  0.38  12.0   \n",
       "5556  7.7  0.31  0.36   4.30  0.026  15.0   87.0  0.99152  3.11  0.48  12.0   \n",
       "\n",
       "      11  12  \n",
       "5773   6   0  \n",
       "5043   5   0  \n",
       "4555   6   0  \n",
       "1753   6   0  \n",
       "4799   7   0  \n",
       "...   ..  ..  \n",
       "3492   5   0  \n",
       "5819   6   0  \n",
       "2814   5   0  \n",
       "4319   7   0  \n",
       "5556   5   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 샘플링\n",
    "df = df_pre.sample(frac=1)  # 랜덤 샘플을 가져오는데, 원본 데이터의 100%를 가져오라는 뜻(frac=0.5로 지정하면 50%만 랜덤으로 가져옴)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.3 ,  0.24,  0.22, ...,  9.3 ,  6.  ,  0.  ],\n",
       "       [ 6.7 ,  0.3 ,  0.44, ...,  9.1 ,  5.  ,  0.  ],\n",
       "       [ 8.1 ,  0.12,  0.38, ..., 12.  ,  6.  ,  0.  ],\n",
       "       ...,\n",
       "       [ 8.  ,  0.28,  0.42, ..., 10.6 ,  5.  ,  0.  ],\n",
       "       [ 7.5 ,  0.26,  0.3 , ..., 12.  ,  7.  ,  0.  ],\n",
       "       [ 7.7 ,  0.31,  0.36, ..., 12.  ,  5.  ,  0.  ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df.values\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:, 0:12]\n",
    "Y = dataset[:, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.3 ,  0.24,  0.22, ...,  0.58,  9.3 ,  6.  ],\n",
       "       [ 6.7 ,  0.3 ,  0.44, ...,  0.53,  9.1 ,  5.  ],\n",
       "       [ 8.1 ,  0.12,  0.38, ...,  0.55, 12.  ,  6.  ],\n",
       "       ...,\n",
       "       [ 8.  ,  0.28,  0.42, ...,  0.43, 10.6 ,  5.  ],\n",
       "       [ 7.5 ,  0.26,  0.3 , ...,  0.38, 12.  ,  7.  ],\n",
       "       [ 7.7 ,  0.31,  0.36, ...,  0.48, 12.  ,  5.  ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "model = Sequential() #바구니\n",
    "model.add(Dense(32,  input_dim=12, activation='relu')) #은닉층 1개\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid')) #아웃풋 레이어 3개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 컴파일\n",
    "model.compile(loss='binary_crossentropy',\n",
    "           optimizer='adam',\n",
    "           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 체크포인트 만들어서 모델 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 폴더 설정\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR): \n",
    "    os.mkdir(MODEL_DIR) # model이라는폴더가 없으면 생성!\n",
    "\n",
    "# 모델 저장 조건 설정          자리(d:정수 f:소수)\n",
    "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 얼리스타핑 만들어서 과적합 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 자동 중단 설정\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 그래프 없이 오차 및 정확도 측정[checkpointer, early_stopping_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25010, saving model to ./model\\01-0.2501.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.25010 to 0.21068, saving model to ./model\\02-0.2107.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.21068 to 0.19546, saving model to ./model\\03-0.1955.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.19546 to 0.18109, saving model to ./model\\04-0.1811.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.18109 to 0.17338, saving model to ./model\\05-0.1734.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.17338 to 0.16893, saving model to ./model\\06-0.1689.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.16893 to 0.16570, saving model to ./model\\07-0.1657.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.16570 to 0.16017, saving model to ./model\\08-0.1602.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.16017 to 0.15678, saving model to ./model\\09-0.1568.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.15678 to 0.15500, saving model to ./model\\10-0.1550.hdf5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.15500 to 0.14759, saving model to ./model\\11-0.1476.hdf5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.14759 to 0.14576, saving model to ./model\\12-0.1458.hdf5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.14576 to 0.14154, saving model to ./model\\13-0.1415.hdf5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.14154 to 0.13717, saving model to ./model\\14-0.1372.hdf5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.13717 to 0.13294, saving model to ./model\\15-0.1329.hdf5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.13294\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.13294\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.13294 to 0.12161, saving model to ./model\\18-0.1216.hdf5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.12161 to 0.12034, saving model to ./model\\19-0.1203.hdf5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.12034 to 0.11981, saving model to ./model\\20-0.1198.hdf5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.11981 to 0.11184, saving model to ./model\\21-0.1118.hdf5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.11184 to 0.11092, saving model to ./model\\22-0.1109.hdf5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.11092 to 0.10945, saving model to ./model\\23-0.1094.hdf5\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10945 to 0.10570, saving model to ./model\\24-0.1057.hdf5\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.10570 to 0.10495, saving model to ./model\\25-0.1050.hdf5\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.10495 to 0.10142, saving model to ./model\\26-0.1014.hdf5\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10142 to 0.09853, saving model to ./model\\27-0.0985.hdf5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.09853\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.09853\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.09853 to 0.09318, saving model to ./model\\30-0.0932.hdf5\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.09318 to 0.09150, saving model to ./model\\31-0.0915.hdf5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.09150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.09150\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.09150 to 0.09072, saving model to ./model\\34-0.0907.hdf5\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.09072 to 0.08755, saving model to ./model\\35-0.0876.hdf5\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.08755 to 0.08600, saving model to ./model\\36-0.0860.hdf5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.08600\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.08600 to 0.08302, saving model to ./model\\38-0.0830.hdf5\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.08302 to 0.08142, saving model to ./model\\39-0.0814.hdf5\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.08142 to 0.08140, saving model to ./model\\40-0.0814.hdf5\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.08140 to 0.07939, saving model to ./model\\41-0.0794.hdf5\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.07939 to 0.07846, saving model to ./model\\42-0.0785.hdf5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.07846\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.07846 to 0.07718, saving model to ./model\\44-0.0772.hdf5\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.07718\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.07718 to 0.07590, saving model to ./model\\46-0.0759.hdf5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.07590\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.07590\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.07590\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.07590\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.07590\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.07590\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.07590 to 0.07412, saving model to ./model\\53-0.0741.hdf5\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.07412\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.07412\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.07412\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.07412\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.07412\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.07412\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.07412\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.07412\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.07412\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.07412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26350adf280>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 실행\n",
    "model.fit(X, Y, validation_split=0.2, epochs=1000, batch_size=200, verbose=0, callbacks=[checkpointer, early_stopping_callback])\n",
    "# 앞서 저장한 모델보다 나은 결과값(테스트 오차값이 감소)이 나올 때만 모델을 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 342us/step - loss: 0.0619 - accuracy: 0.9818\n",
      "\n",
      " Accuracy: 0.9818\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 그래프로 테스트셋 오차, 학습셋 정확도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0483 - accuracy: 0.9900\n",
      "Epoch 00001: val_loss did not improve from 0.07412\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0584 - accuracy: 0.9838 - val_loss: 0.0765 - val_accuracy: 0.9808\n",
      "Epoch 2/1000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0383 - accuracy: 0.9900\n",
      "Epoch 00002: val_loss did not improve from 0.07412\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0593 - accuracy: 0.9827 - val_loss: 0.0794 - val_accuracy: 0.9815\n",
      "Epoch 3/1000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1189 - accuracy: 0.9600\n",
      "Epoch 00003: val_loss did not improve from 0.07412\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.9817 - val_loss: 0.0808 - val_accuracy: 0.9800\n",
      "Epoch 4/1000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0557 - accuracy: 0.9850\n",
      "Epoch 00004: val_loss did not improve from 0.07412\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9813 - val_loss: 0.0861 - val_accuracy: 0.9762\n",
      "Epoch 5/1000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0745 - accuracy: 0.9750\n",
      "Epoch 00005: val_loss did not improve from 0.07412\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.9802 - val_loss: 0.0804 - val_accuracy: 0.9808\n",
      "Epoch 6/1000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0749 - accuracy: 0.9850\n",
      "Epoch 00006: val_loss did not improve from 0.07412\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.9811 - val_loss: 0.1009 - val_accuracy: 0.9708\n",
      "Epoch 7/1000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0752 - accuracy: 0.9750\n",
      "Epoch 00007: val_loss did not improve from 0.07412\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0604 - accuracy: 0.9827 - val_loss: 0.0776 - val_accuracy: 0.9831\n",
      "Epoch 8/1000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0854 - accuracy: 0.9800\n",
      "Epoch 00008: val_loss did not improve from 0.07412\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0569 - accuracy: 0.9817 - val_loss: 0.0862 - val_accuracy: 0.9785\n",
      "Epoch 9/1000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0749 - accuracy: 0.9850\n",
      "Epoch 00009: val_loss did not improve from 0.07412\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.9833 - val_loss: 0.0876 - val_accuracy: 0.9800\n",
      "Epoch 10/1000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0531 - accuracy: 0.9750\n",
      "Epoch 00010: val_loss did not improve from 0.07412\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0561 - accuracy: 0.9831 - val_loss: 0.0787 - val_accuracy: 0.9800\n",
      "Epoch 11/1000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0346 - accuracy: 0.9900\n",
      "Epoch 00011: val_loss did not improve from 0.07412\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0593 - accuracy: 0.9811 - val_loss: 0.0810 - val_accuracy: 0.9831\n"
     ]
    }
   ],
   "source": [
    "# 모델 실행 및 저장\n",
    "history = model.fit(X, Y, validation_split=0.2, epochs=1000, batch_size=200, verbose=1, callbacks=[checkpointer, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_vloss에 테스트셋 오차를 저장\n",
    "y_vloss = history.history['val_loss']\n",
    "\n",
    "# y_acc 에 학습셋 정확도를 저장\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "# y_val_accuracy에 테스트셋 정확도를 저장\n",
    "y_val_accuracy = history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9807692170143127,\n",
       " 0.9815384745597839,\n",
       " 0.9800000190734863,\n",
       " 0.9761538505554199,\n",
       " 0.9807692170143127,\n",
       " 0.9707692265510559,\n",
       " 0.9830769300460815,\n",
       " 0.9784615635871887,\n",
       " 0.9800000190734863,\n",
       " 0.9800000190734863,\n",
       " 0.9830769300460815]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Accuracy: 0.9831\n"
     ]
    }
   ],
   "source": [
    "# x값을 지정하고 테스트셋 정확도를 파란색으로, 학습셋 오차를 빨간색으로 표시\n",
    "x_len = numpy.arange(len(y_acc))\n",
    "\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3, label='Trainset_accuracy')\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3, label='Testset_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('score')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 테스트 정확도 출력\n",
    "print(\"\\n Test Accuracy: %.4f\" % y_val_accuracy[-1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epoch가 진행될수록 학습 셋 정확도는 1에 수렴하고, 테스트셋 오차는 점점 0에 수렴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
